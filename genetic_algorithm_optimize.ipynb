{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "genetic_algorithm_optimize",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPDKpzAyB5jSbDBjhDNLnvg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/douglasamante/genetic_algorithm/blob/main/genetic_algorithm_optimize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTC2TSBBpKnH"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdwTSOf0Easj"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout\n",
        "from keras.layers.convolutional import Convolution2D\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.models import Model\n",
        "from keras.models import load_model"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZaxrdyMFEiH"
      },
      "source": [
        "# Some Relevants Concepts about this distribuition\n",
        "\n",
        "Train Mnist data have 6000 instances, that is, there are 6000 numbers between 0-9. \n",
        "\n",
        "For instance, in command line 6 I will print the first 10 labels about the training [5 0 4 1 9 2 1 3 1 4]. You can do other tests, if you want.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5Ur1K-lpdOv",
        "outputId": "be6cc5d4-3b91-4bf5-aa3f-19d525840b5b"
      },
      "source": [
        "# This part is responsable for setup train and tests using mnist dataset\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(\"Training Label Shape: \", y_train.shape) # (60000,) -- 60000 numbers (all 0-9)\n",
        "\n",
        "\n",
        "print(\"These is the first 10 training labels: \", y_train[:10]) "
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Label Shape:  (60000,)\n",
            "These is the first 10 training labels:  [5 0 4 1 9 2 1 3 1 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtaXCSTUsjHG",
        "outputId": "384f4e11-74e5-47bf-b257-a2b6178f340a"
      },
      "source": [
        "# Convert to \"one-hot\" vectors using the to_categorical function, that is, y_train*10\n",
        "\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# splitting dataset for faster learning and debugging\n",
        "X_train, _,_,_,_,_ = np.split(X_train,6) \n",
        "y_train, _,_,_,_,_ = np.split(y_train,6)\n",
        "\n",
        "# input and output shape\n",
        "input_shape = X_train[1].shape\n",
        "\n",
        "# Flatten the images\n",
        "image_vector_size = 28*28\n",
        "print(image_vector_size)\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], image_vector_size)\n",
        "X_test = X_test.reshape(X_test.shape[0], image_vector_size)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(X_train.shape)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "784\n",
            "(10000, 10)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROIBEfZ8pmcm"
      },
      "source": [
        "\n",
        "#**DNA(DOUGLAS NEURAL ARCHITECTURE) PARAMETER TO OPTIMIZE** \n",
        "\n",
        "## **Depth** = [1,2,3,4,5,6,7,8,9,10]\n",
        "\n",
        "---\n",
        "\n",
        "## **Neurons_per_layer** = [16,32,64,128,256,512,1024]\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## **Activations** = [\"tanh\",\"softmax\",\"relu\",\"sigmoid\"] \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## **Optimizer** = [\"sgd\",\"rmsprop\",\"adagrad\",\"adadelta\",\"adam\",\"adamax\",\"nadam\"],\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **Losses** =[\"mean_squared_error\",\"mean_absolute_error\",\"mean_absolute_percentage_error\",\"mean_squared_logarithmic_error\",\"squared_hinge\",\"hinge\",\"categorical_hinge\",\"logcosh\",\"categorical_crossentropy\",\"sparse_categorical_crossentropy\",\"binary_crossentropy\",\"kullback_leibler_divergence\",\"poisson\",\"cosine_proximity\"]\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylUDZCX6phgD"
      },
      "source": [
        "# DNA[0] = depth\n",
        "# DNA[1] = neurons_per_layer\n",
        "# DNA[2] = activations\n",
        "# DNA[3] = optimizer\n",
        "# DNA[4] = losses\n",
        "DNA_parameter = [[5,6,7,8,9,10],\n",
        "                 [16,32,64,128,256,512,1024],\n",
        "                 [\"tanh\",\"softmax\",\"relu\",\"sigmoid\",\"elu\",\"selu\",\"softplus\",\"softsign\",\"hard_sigmoid\",\"linear\"], \n",
        "                 [\"sgd\",\"rmsprop\",\"adagrad\",\"adadelta\",\"adam\",\"adamax\",\"nadam\"],\n",
        "                 [\"mean_squared_error\",\"mean_absolute_error\",\"mean_absolute_percentage_error\",\"mean_squared_logarithmic_error\",\"squared_hinge\",\"hinge\",\"categorical_hinge\",\"logcosh\",\"categorical_crossentropy\",\"binary_crossentropy\",\"kullback_leibler_divergence\",\"poisson\",\"cosine_proximity\"] #\"sparse_categorical_crossentropy\",\n",
        "                ]"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF8hpC2yPoCM"
      },
      "source": [
        "# DOUGLAS NEURAL ARCHITECTURE STRUCTURE:\n",
        "         architecture_DNA[0] = Depth\n",
        "         architecture_DNA[1] = Input_layer with: [0] = neurons, [1] = activation\n",
        "         architecture_DNA[2 to Depth-1*] = Hidden layer with: [0] = neurons, [1] = activation   \n",
        "         architecture_DNA[Depth] = Output layer activation\n",
        "         architecture_DNA[-1] = Hyperparameter with: [0] = loss, [1] = optimizer\n",
        "        [link text]\n",
        "\n",
        "         *Depth-1 last hidden layer since last layer is output layer\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni48IXXsRul2"
      },
      "source": [
        "### UNFOLD CHILDREN ARTIFICIAL NEURAL NETWORK:                   \n",
        "         children_ANN[0] = Depth                                                          \n",
        "         children_ANN[1] = Input_layer with: [0] = neurons, [1] = activation              \n",
        "         children_ANN[2 to Depth-1*] = Hidden layer with: [0] = neurons, [1] = activation \n",
        "         children_ANN[Depth] = Output layer activation                                    \n",
        "         children_ANN[-1] = Hyperparameter with: [0] = loss, [1] = optimizer              \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmilQXz9p386"
      },
      "source": [
        "class Neural_Network:\n",
        "\n",
        "    def __init__(self, input_shape, classes, DNA_param, epochs):\n",
        "        \n",
        "        self.architecture_DNA = [] # to save current parameters\n",
        "        self.fitness = []\n",
        "        self.acc_history = []\n",
        "        self.input_shape = input_shape \n",
        "        self.classes = classes\n",
        "        self.epochs = epochs\n",
        "\n",
        "        \n",
        "        # unfold DNA_parameters:\n",
        "        depth = DNA_param[0]\n",
        "        neurons_per_layer = DNA_param[1]\n",
        "        activations = DNA_param[2]\n",
        "        optimizer = DNA_param[3]\n",
        "        losses = DNA_param[4]\n",
        "        \n",
        "        model = Sequential()\n",
        "\n",
        "        # Building the init network with random choices: \n",
        "        network_depth = np.random.choice(depth)\n",
        "        self.architecture_DNA.append(network_depth)\n",
        "\n",
        "        for i in range(network_depth):\n",
        "            if i == 0:\n",
        "                neurons = np.random.choice(neurons_per_layer)\n",
        "                activation  = np.random.choice(activations)\n",
        "                self.architecture_DNA.append([neurons, activation])\n",
        "\n",
        "                model.add(Dense(neurons,input_shape = (self.input_shape,), activation  = activation))\n",
        "            if i == network_depth - 1:\n",
        "                activation  = np.random.choice(activations)\n",
        "                self.architecture_DNA.append(activation)\n",
        "\n",
        "                model.add(Dense(self.classes, activation = activation ))\n",
        "            else:\n",
        "                neurons = np.random.choice(neurons_per_layer)\n",
        "                activation  = np.random.choice(activations)\n",
        "                self.architecture_DNA.append([neurons,activation])\n",
        "\n",
        "                model.add(Dense(neurons, activation  = activation))\n",
        "        \n",
        "        loss=np.random.choice(losses)\n",
        "        optimizer=np.random.choice(optimizer)\n",
        "        self.architecture_DNA.append([loss,optimizer])\n",
        "\n",
        "        model.compile(loss=loss, optimizer= optimizer, metrics=['accuracy'])\n",
        "        self.model = model\n",
        "        \n",
        "      \n",
        "    def create_children(self, children_DNA):\n",
        "        model = Sequential()\n",
        "        print()\n",
        "        print(\"DNA Length: \", len(children_DNA))\n",
        "        children_depth = children_DNA[0]\n",
        "        print(\"DNA Depth: \", children_depth)\n",
        "        print()\n",
        "        print(\"New Values of Parameters :\")\n",
        "        print(children_DNA)\n",
        "\n",
        "        for i in range(children_depth):\n",
        "            if i == 0:\n",
        "                #Input Layer\n",
        "\n",
        "                model.add(Dense(children_DNA[1][0],input_shape = (self.input_shape,), activation = children_DNA[1][1]))\n",
        "            if i == children_depth -1:\n",
        "\n",
        "                model.add(Dense(self.classes, activation = children_DNA[children_depth]))\n",
        "            else:\n",
        "                #print(children_DNA[i+1])\n",
        "                if i != children_depth -1:\n",
        "                    model.add(Dense(children_DNA[i+1][0], activation = children_DNA[i+1][1]))\n",
        "\n",
        "        model.compile(loss = children_DNA[-1][0], optimizer = children_DNA[-1][1], metrics=['accuracy'])\n",
        "        self.model = model\n",
        "        self.architecture_DNA = children_DNA\n",
        "        \n",
        "        \n",
        "    def give_fitness(self):\n",
        "        return self.fitness\n",
        "    \n",
        "        \n",
        "    def train(self):\n",
        "        \n",
        "        self.model.fit(X_train,y_train, batch_size = 32, epochs = self.epochs, verbose = 1,shuffle = True) #, validation_data =(X_test, y_test)\n",
        "        \n",
        " \n",
        "    def test(self):\n",
        "        loss, acc = self.model.evaluate(X_test,y_test)\n",
        "        self.fitness = acc\n",
        "        self.acc_history.append(acc)\n",
        "    \n",
        "    def give_DNA(self):\n",
        "        return self.architecture_DNA\n",
        "    \n",
        "    def architecture(self):\n",
        "        self.model.summary()"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ_CoToFqVEd"
      },
      "source": [
        "class GeneticAlgorithm:\n",
        "\n",
        "    def __init__(self, population_size, mutation_rate, generations = 50, Epochs = 2):\n",
        "\n",
        "        self.population_size = population_size\n",
        "        self.mutation_rate = mutation_rate\n",
        "        self.generations = generations\n",
        "        self.training_epochs = Epochs\n",
        "        self.population = None\n",
        "        self.children_population_DNA = []\n",
        "        self.acces = []\n",
        "        self.norm_acces = []\n",
        "        \n",
        "    def create_population(self):\n",
        "        self.population = [Neural_Network(image_vector_size, num_classes, DNA_parameter,self.training_epochs) for i in range(self.population_size)]\n",
        "    \n",
        "    def train_generation(self):\n",
        "        for member in self.population:\n",
        "                member.train()\n",
        "                \n",
        "    def predict(self):\n",
        "        for member in self.population:\n",
        "                member.test()\n",
        "                self.acc.append(member.give_fitness())\n",
        "    \n",
        "    def normalize(self):\n",
        "        sum_ = sum(self.acc)\n",
        "        self.norm_acc = [i/sum_ for i in self.acc] \n",
        "        #print(\"\\nNormalization sum: \",sum(self.norm_acc))\n",
        "        #assert sum(self.norm_acc) == 1\n",
        "        \n",
        "    def clear_losses(self):\n",
        "        self.norm_acc = []\n",
        "        self.acc = []\n",
        "        \n",
        "    def mutate(self):\n",
        "        for child_DNA in self.children_population_DNA:\n",
        "\n",
        "            for i in range(len(child_DNA)):\n",
        "              \n",
        "                if np.random.random() < self.mutation_rate:\n",
        "                    print(\"\\nMutation will hapeness now!\")\n",
        "                    time.sleep(10)\n",
        "                    if i == 0:\n",
        "                        new_depth = np.random.choice(DNA_parameter[0])\n",
        "                        child_DNA[0] = new_depth\n",
        "                    \n",
        "                    if i == len(child_DNA)-2:\n",
        "                        new_output_activation = np.random.choice(DNA_parameter[2])\n",
        "                        child_DNA[-2] = new_output_activation\n",
        "                    \n",
        "                    if i == len(child_DNA)-1:\n",
        "                        # random flip if loss or activation shall be changed\n",
        "\n",
        "                        if np.random.random() < 0.5:\n",
        "                            new_loss = np.random.choice(DNA_parameter[4])\n",
        "\n",
        "                            child_DNA[-1][0] = new_loss\n",
        "                        else:\n",
        "                            new_optimizer = np.random.choice(DNA_parameter[3])\n",
        "                            child_DNA[-1][1] = new_optimizer\n",
        "                    if i != 0 and i !=len(child_DNA)-2 and i != len(child_DNA)-1:\n",
        "                    \n",
        "                        if np.random.random() < 0.33:\n",
        "                            #print(child_DNA[i][1])\n",
        "                            new_activation = np.random.choice(DNA_parameter[2])\n",
        "                            #print(new_activation)\n",
        "                            child_DNA[i][1] = new_activation\n",
        "\n",
        "                        else:\n",
        "                            #print(child_DNA[i][0])\n",
        "                            new_neuron_count = np.random.choice(DNA_parameter[1])\n",
        "                            child_DNA[i][0] = new_neuron_count\n",
        "                            #print(new_neuron_count)\n",
        "\n",
        "                    print(\"After mutation \", child_DNA)\n",
        "\n",
        "    def reproduction(self):\n",
        "\n",
        "        \"\"\" \n",
        "        Reproduction through midpoint crossover method \n",
        "        \"\"\"\n",
        "\n",
        "        population_idx = [i for i in range(len(self.population))]\n",
        "        for i in range(len(self.population)):\n",
        "        #selects two parents probabilistic accroding to the fitness score\n",
        "\n",
        "            if sum(self.norm_acc) != 0:\n",
        "                parent1 = np.random.choice(population_idx, p = self.norm_acc)\n",
        "                parent2 = np.random.choice(population_idx, p = self.norm_acc)\n",
        "\n",
        "            else:\n",
        "              # if there are no \"best\" parents choose randomly \n",
        "                parent1 = np.random.choice(population_idx)\n",
        "                parent2 = np.random.choice(population_idx)\n",
        "\n",
        "            # picking random midpoint for crossing over name/DNA\n",
        "            parent1_DNA = self.population[parent1].give_DNA()\n",
        "            parent2_DNA = self.population[parent2].give_DNA()\n",
        "            #print(parent1_DNA)\n",
        "            \n",
        "            mid_point_1 = np.random.choice([i for i in range(2,len(parent1_DNA)-2)])\n",
        "            mid_point_2 = np.random.choice([i for i in range(2,len(parent2_DNA)-2)])\n",
        "            # adding DNA-Sequences of the parents to final DNA\n",
        "            child_DNA = parent1_DNA[:mid_point_1] + parent2_DNA[mid_point_2:]\n",
        "            new_nn_depth = len(child_DNA)-2 # minus 2 because of depth parameter[0] and loss parameter[-1]\n",
        "            child_DNA[0] = new_nn_depth\n",
        "            self.children_population_DNA.append(child_DNA)\n",
        "\n",
        "        # old population gets the new and proper weights\n",
        "        self.mutate()\n",
        "\n",
        "        keras.backend.clear_session() ## delete old models to free memory\n",
        "\n",
        "        for i in range(len(self.population)):\n",
        "            self.population[i].create_children(self.children_population_DNA[i])\n",
        "        \n",
        "        \n",
        "    \n",
        "    def run_evolution(self):\n",
        "        for episode in range(self.generations):\n",
        "\n",
        "            print(\"\\n--- Generation {} ---\".format(episode))\n",
        "\n",
        "            self.clear_losses()\n",
        "            self.train_generation()\n",
        "            self.predict()\n",
        "\n",
        "            if episode != self.generations -1:\n",
        "                self.normalize()\n",
        "                self.reproduction()\n",
        "                \n",
        "            else:\n",
        "                pass\n",
        "            self.children_population_DNA = []\n",
        "\n",
        "        # plotting all the history:\n",
        "\n",
        "        for a in range(self.generations):\n",
        "            for member in self.population:\n",
        "\n",
        "                plt.plot(member.acc_history)\n",
        "        plt.xlabel(\"Number of Generations\")\n",
        "        plt.ylabel(\"Accuracy Rate\")\n",
        "        plt.show()"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yWRRQhviqgwx",
        "outputId": "0d09670b-15c5-4a4f-c1f8-c01d5020228b"
      },
      "source": [
        "GA = GeneticAlgorithm(population_size = 4, mutation_rate = 0.03, generations = 3, Epochs=1)\n",
        "GA.create_population()\n",
        "GA.run_evolution()"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--- Generation 0 ---\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 217844655.2357 - accuracy: 0.1067\n",
            "313/313 [==============================] - 8s 24ms/step - loss: 0.3303 - accuracy: 0.0998\n",
            "313/313 [==============================] - 2s 3ms/step - loss: 1.3735 - accuracy: 0.0976\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3526 - accuracy: 0.1701\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 56499568.0000 - accuracy: 0.1135\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.3302 - accuracy: 0.1135\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.3516 - accuracy: 0.0958\n",
            "313/313 [==============================] - 1s 1ms/step - loss: 0.2791 - accuracy: 0.3323\n",
            "\n",
            "Mutation will hapeness now!\n",
            "After mutation  [18, [256, 'hard_sigmoid'], [64, 'relu'], [128, 'softsign'], [16, 'elu'], [32, 'relu'], [512, 'elu'], [1024, 'softmax'], [1024, 'elu'], [512, 'relu'], [32, 'linear'], [16, 'tanh'], [128, 'elu'], [32, 'hard_sigmoid'], [128, 'softsign'], [64, 'tanh'], [256, 'softsign'], [32, 'linear'], 'softplus', ['poisson', 'adamax']]\n",
            "\n",
            "DNA Length:  20\n",
            "DNA Depth:  18\n",
            "\n",
            "News Parameters Values:\n",
            "[18, [256, 'hard_sigmoid'], [64, 'relu'], [128, 'softsign'], [16, 'elu'], [32, 'relu'], [512, 'elu'], [1024, 'softmax'], [1024, 'elu'], [512, 'relu'], [32, 'linear'], [16, 'tanh'], [128, 'elu'], [32, 'hard_sigmoid'], [128, 'softsign'], [64, 'tanh'], [256, 'softsign'], [32, 'linear'], 'softplus', ['poisson', 'adamax']]\n",
            "\n",
            "DNA Length:  13\n",
            "DNA Depth:  11\n",
            "\n",
            "News Parameters Values:\n",
            "[11, [1024, 'selu'], [128, 'sigmoid'], [16, 'softmax'], [512, 'softplus'], [256, 'tanh'], [16, 'softsign'], [256, 'tanh'], [64, 'selu'], [512, 'softplus'], [256, 'tanh'], 'softsign', ['mean_absolute_percentage_error', 'adamax']]\n",
            "\n",
            "DNA Length:  13\n",
            "DNA Depth:  11\n",
            "\n",
            "News Parameters Values:\n",
            "[11, [32, 'softmax'], [64, 'sigmoid'], [32, 'linear'], [16, 'tanh'], [128, 'elu'], [32, 'hard_sigmoid'], [128, 'softsign'], [64, 'tanh'], [256, 'softsign'], [32, 'linear'], 'softplus', ['poisson', 'adamax']]\n",
            "\n",
            "DNA Length:  12\n",
            "DNA Depth:  10\n",
            "\n",
            "News Parameters Values:\n",
            "[10, [32, 'softmax'], [32, 'linear'], [16, 'tanh'], [128, 'elu'], [32, 'hard_sigmoid'], [128, 'softsign'], [64, 'tanh'], [256, 'softsign'], [32, 'linear'], 'softplus', ['poisson', 'adamax']]\n",
            "\n",
            "--- Generation 1 ---\n",
            "313/313 [==============================] - 12s 34ms/step - loss: 0.3720 - accuracy: 0.0948\n",
            "313/313 [==============================] - 9s 27ms/step - loss: 203255143.0828 - accuracy: 0.1003\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3598 - accuracy: 0.0964\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3677 - accuracy: 0.1100\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.3309 - accuracy: 0.1135\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 47632724.0000 - accuracy: 0.1028\n",
            "313/313 [==============================] - 1s 1ms/step - loss: 0.3320 - accuracy: 0.1028\n",
            "313/313 [==============================] - 1s 1ms/step - loss: 0.3305 - accuracy: 0.1135\n",
            "\n",
            "Mutation will hapeness now!\n",
            "After mutation  [10, [256, 'selu'], [128, 'sigmoid'], [16, 'softmax'], [128, 'elu'], [32, 'hard_sigmoid'], [128, 'softsign'], [64, 'tanh'], [256, 'softsign'], [32, 'linear'], 'softplus', ['poisson', 'adamax']]\n",
            "\n",
            "Mutation will hapeness now!\n",
            "After mutation  [10, [256, 'hard_sigmoid'], [64, 'relu'], [128, 'softsign'], [16, 'elu'], [32, 'relu'], [512, 'elu'], [1024, 'softmax'], [1024, 'elu'], [16, 'elu'], [32, 'relu'], [512, 'elu'], [1024, 'softmax'], [1024, 'elu'], [512, 'relu'], [32, 'linear'], [16, 'tanh'], [128, 'elu'], [32, 'hard_sigmoid'], [128, 'softsign'], [64, 'tanh'], [256, 'softsign'], [32, 'linear'], 'softplus', ['poisson', 'adamax']]\n",
            "\n",
            "Mutation will hapeness now!\n",
            "After mutation  [22, [32, 'softmax'], [32, 'linear'], [128, 'tanh'], [128, 'elu'], [32, 'hard_sigmoid'], [128, 'softsign'], [64, 'tanh'], [256, 'softsign'], [32, 'relu'], [512, 'elu'], [1024, 'softmax'], [1024, 'elu'], [512, 'relu'], [32, 'linear'], [128, 'tanh'], [128, 'elu'], [32, 'hard_sigmoid'], [128, 'softsign'], [64, 'tanh'], [256, 'softsign'], [32, 'linear'], 'softplus', ['poisson', 'adamax']]\n",
            "\n",
            "Mutation will hapeness now!\n",
            "After mutation  [22, [32, 'softmax'], [32, 'linear'], [128, 'tanh'], [128, 'elu'], [32, 'hard_sigmoid'], [128, 'softsign'], [16, 'tanh'], [256, 'softsign'], [32, 'relu'], [512, 'elu'], [1024, 'softmax'], [1024, 'elu'], [512, 'relu'], [32, 'linear'], [128, 'tanh'], [128, 'elu'], [32, 'hard_sigmoid'], [128, 'softsign'], [16, 'tanh'], [256, 'softsign'], [32, 'linear'], 'softplus', ['poisson', 'adamax']]\n",
            "\n",
            "DNA Length:  12\n",
            "DNA Depth:  10\n",
            "\n",
            "News Parameters Values:\n",
            "[10, [256, 'selu'], [128, 'sigmoid'], [16, 'softmax'], [128, 'elu'], [32, 'hard_sigmoid'], [128, 'softsign'], [16, 'tanh'], [256, 'softsign'], [32, 'linear'], 'softplus', ['poisson', 'adamax']]\n",
            "\n",
            "DNA Length:  25\n",
            "DNA Depth:  10\n",
            "\n",
            "News Parameters Values:\n",
            "[10, [256, 'hard_sigmoid'], [64, 'relu'], [128, 'softsign'], [16, 'elu'], [32, 'relu'], [512, 'elu'], [1024, 'softmax'], [1024, 'elu'], [16, 'elu'], [32, 'relu'], [512, 'elu'], [1024, 'softmax'], [1024, 'elu'], [512, 'relu'], [32, 'linear'], [128, 'tanh'], [128, 'elu'], [32, 'hard_sigmoid'], [128, 'softsign'], [16, 'tanh'], [256, 'softsign'], [32, 'linear'], 'softplus', ['poisson', 'adamax']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-153-ae4f10601a09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mGA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGeneticAlgorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutation_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEpochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mGA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_population\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mGA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_evolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-150-be685f79b1b1>\u001b[0m in \u001b[0;36mrun_evolution\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreproduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-150-be685f79b1b1>\u001b[0m in \u001b[0;36mreproduction\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren_population_DNA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-152-c21aeab17ec6>\u001b[0m in \u001b[0;36mcreate_children\u001b[0;34m(self, children_DNA)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mchildren_depth\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchildren_DNA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchildren_depth\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;31m#print(children_DNA[i+1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/activations.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    579\u001b[0m     raise TypeError(\n\u001b[1;32m    580\u001b[0m         'Could not interpret activation function identifier: {}'.format(\n\u001b[0;32m--> 581\u001b[0;31m             identifier))\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: Could not interpret activation function identifier: [32, 'relu']"
          ]
        }
      ]
    }
  ]
}